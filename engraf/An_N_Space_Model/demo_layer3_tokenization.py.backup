#!/usr/bin/env python3
"""
LATN Layer 3 Tokenization Demo

This demo shows the complete LATN pipeline through Layer 3, demonstrating:
1. Layer 1: Lexical tokenization
2. Layer 2: NP tokenization with semantic grounding (NP ‚Üí SO)
3. Layer 3: PP tokenization with spatial validation (PP ‚Üí PPSO)

The demo focuses on the final results rather than intermediate processing steps.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from engraf.lexer.latn_layer_executor import LATNLayerExecutor
from engraf.visualizer.scene.scene_model import SceneModel
from engraf.visualizer.scene.scene_object import SceneObject
from engraf.lexer.vector_space import vector_from_features
from engraf.utils.debug import set_debug


def setup_demo_scene():
    """Create a demo scene with positioned objects for spatial relationships."""
    scene = SceneModel()
    
    # Create objects with specific positions for spatial testing
    # Base table
    table_vector = vector_from_features("noun", locX=0.0, locY=-1.0, locZ=0.0, scaleX=2.0, scaleY=0.2, scaleZ=2.0)
    table = SceneObject("table", table_vector, object_id="table_1")
    scene.add_object(table)
    
    # Red cube on the table
    red_cube_vector = vector_from_features("noun", red=1.0, locX=0.0, locY=0.0, locZ=0.0)
    red_cube = SceneObject("cube", red_cube_vector, object_id="red_cube_1")
    scene.add_object(red_cube)
    
    # Blue sphere to the right of the cube
    blue_sphere_vector = vector_from_features("noun", blue=1.0, locX=2.0, locY=0.0, locZ=0.0)
    blue_sphere = SceneObject("sphere", blue_sphere_vector, object_id="blue_sphere_1")
    scene.add_object(blue_sphere)
    
    # Green cylinder behind the cube
    green_cylinder_vector = vector_from_features("noun", green=1.0, locX=0.0, locY=0.0, locZ=-2.0)
    green_cylinder = SceneObject("cylinder", green_cylinder_vector, object_id="green_cylinder_1")
    scene.add_object(green_cylinder)
    
    return scene


def demo_layer3_spatial_relationships():
    """Demonstrate Layer 3 processing of spatial prepositional phrases."""
    print("=== Layer 3 Spatial Relationship Processing ===")
    print("Testing prepositional phrases that reference scene objects")
    print()
    
    # Set up the scene with positioned objects
    scene = SceneModel()
    scene.add_object('cube', {'x': 0, 'y': 0, 'z': 0}, object_type='cube', attributes={'color': 'red'})
    scene.add_object('sphere', {'x': 2, 'y': 0, 'z': 0}, object_type='sphere', attributes={'color': 'blue'})
    scene.add_object('cylinder', {'x': 1, 'y': 1, 'z': 0}, object_type='cylinder', attributes={'color': 'green'})
    scene.add_object('table', {'x': 1, 'y': 0, 'z': -1}, object_type='table')
    
    executor = LATNLayerExecutor(scene)
    
    test_phrases = [
        "the cube above the table",
        "the sphere right of the cube", 
        "the cylinder behind the cube",
        "move the cube below the sphere",
        "place a box above the table"
    ]
    
    for phrase in test_phrases:
        print(f"Phrase: '{phrase}'")
        
        # First show Layer 3 tokenization only (no grounding)
        print("  üìù Layer 3 Tokenization (no grounding):")
        result_tokenization = executor.execute_layer3(phrase, enable_semantic_grounding=False)
        if result_tokenization.success and result_tokenization.hypotheses:
            hypothesis = result_tokenization.hypotheses[0]
            print(f"    Tokens: {[tok.word for tok in hypothesis.tokens]}")
            pp_tokens = [tok for tok in hypothesis.tokens if tok.isa("PP")]
            if pp_tokens:
                print(f"    ‚Üí PP tokens: {[tok.word for tok in pp_tokens]}")
        
        # Then show Layer 3 with grounding (creates PPSOs)  
        print("  üéØ Layer 3 with Grounding (creates PPSOs):")
        result_grounding = executor.execute_layer3(phrase, enable_semantic_grounding=True)
        if result_grounding.success and result_grounding.hypotheses:
            hypothesis = result_grounding.hypotheses[0]
            print(f"    Tokens: {[tok.word for tok in hypothesis.tokens]}")
            
            # Check for different token types
            so_tokens = [tok for tok in hypothesis.tokens if tok.isa("SO")]
            pp_tokens = [tok for tok in hypothesis.tokens if tok.isa("PP")]
            ppso_tokens = [tok for tok in hypothesis.tokens if tok.isa("PPSO")]
            
            if so_tokens:
                print(f"    ‚Üí Scene Objects (SO): {[tok.word for tok in so_tokens]}")
            if pp_tokens:
                print(f"    ‚Üí Prepositional Phrases (PP): {[tok.word for tok in pp_tokens]}")
            if ppso_tokens:
                print(f"    ‚Üí Spatial PPSOs: {[tok.word for tok in ppso_tokens]}")
                
        print()


def demo_layer3_complex_structures():
    """Demonstrate Layer 3 processing of complex grammatical structures."""
    print("=== Layer 3 Complex Structure Processing ===")
    print("Testing complex sentences with multiple spatial relationships")
    print()
    
    scene = setup_demo_scene()
    executor = LATNLayerExecutor(scene_model=scene)
    
    test_phrases = [
        "draw a red cube above the table",
        "move the cube below the table",
        "create a sphere above the cube behind the table"
    ]
    
    for phrase in test_phrases:
        print(f"Phrase: '{phrase}'")
        
        # Execute full Layer 3 pipeline
        result = executor.execute_layer3(phrase, enable_semantic_grounding=True)
        if not result.success:
            print(f"  ‚ùå Processing failed: {result.description}")
            continue
            
        # Show detailed analysis of the result
        if result.hypotheses:
            hypothesis = result.hypotheses[0]
            print(f"  Confidence: {hypothesis.confidence:.3f}")
            print(f"  Token count: {len(hypothesis.tokens)}")
            
            # Group tokens by type
            token_groups = {
                "Verbs": [tok for tok in hypothesis.tokens if tok.isa("verb")],
                "Scene Objects": [tok for tok in hypothesis.tokens if tok.isa("SO")],
                "Spatial PPSOs": [tok for tok in hypothesis.tokens if tok.isa("PPSO")], 
                "Other PPs": [tok for tok in hypothesis.tokens if tok.isa("PP") and not tok.isa("PPSO")],
                "Other": [tok for tok in hypothesis.tokens if not any([tok.isa("verb"), tok.isa("SO"), tok.isa("PP")])]
            }
            
            for group_name, tokens in token_groups.items():
                if tokens:
                    print(f"  {group_name}: {[tok.word for tok in tokens]}")
                    
        print()


def demo_layer3_grounding_failures():
    """Demonstrate how Layer 3 handles phrases that don't ground properly."""
    print("=== Layer 3 Grounding Edge Cases ===")
    print("Testing phrases with non-existent objects or invalid spatial relationships")
    print()
    
    scene = setup_demo_scene()
    executor = LATNLayerExecutor(scene_model=scene)
    
    test_phrases = [
        "the pyramid above the table",      # pyramid doesn't exist
        "the cube above the sphere",        # valid spatial relationship but might be invalid spatially
        "the table above the cube"          # reversed spatial relationship
    ]
    
    for phrase in test_phrases:
        print(f"Phrase: '{phrase}'")
        
        # Execute full Layer 3 pipeline
        result = executor.execute_layer3(phrase, enable_semantic_grounding=True)
        if not result.success:
            print(f"  ‚ùå Processing failed: {result.description}")
        else:
            hypothesis = result.hypotheses[0] if result.hypotheses else None
            if hypothesis:
                so_count = len([tok for tok in hypothesis.tokens if tok.isa("SO")])
                pp_count = len([tok for tok in hypothesis.tokens if tok.isa("PP")])
                ppso_count = len([tok for tok in hypothesis.tokens if tok.isa("PPSO")])
                
                print(f"  ‚úì Processed: {so_count} SOs, {pp_count} PPs, {ppso_count} PPSOs")
                print(f"  Tokens: {[tok.word for tok in hypothesis.tokens]}")
            else:
                print(f"  ‚ùå No hypotheses generated")
                
        print()


def main():
    """Run all Layer 3 tokenization demonstrations."""
    set_debug(False)
    
    print("LATN Layer 3 Tokenization Demo")
    print("==============================")
    print()
    print("This demo shows the complete LATN pipeline through Layer 3,")
    print("demonstrating how spatial prepositional phrases are processed")
    print("and converted to PrepositionalPhraseSceneObject (PPSO) tokens.")
    print()
    
    # Run the demonstrations
    demo_layer3_spatial_relationships()
    demo_layer3_complex_structures()
    demo_layer3_grounding_failures()
    
    print("Demo complete! Layer 3 processing enables spatial reasoning")
    print("about relationships between grounded scene objects.")


if __name__ == "__main__":
    main()
