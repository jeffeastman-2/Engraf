#!/usr/bin/env python3
"""
LATN Layer 3: Simplified PP ATN Tests

These tests celebrate the beautiful simplicity of the Layer 3 PP ATN:
  PREP -> NounPhrase token -> END

No more complex subnetwork parsing! The PP ATN in Layer 3 is elegantly simple
because it works directly with NounPhrase tokens from Layer 2.
"""

import pytest
from engraf.lexer.token_stream import TokenStream
from engraf.atn.subnet_pp import run_pp
from engraf.lexer.latn_tokenizer import latn_tokenize_best as tokenize
from engraf.lexer.vector_space import VectorSpace, vector_from_features
from engraf.pos.noun_phrase import NounPhrase


def create_np_token(text, **attributes):
    """Helper to create a NounPhrase token for testing."""
    # Parse the NP first to get proper structure
    from engraf.atn.subnet_np import run_np
    np_tokens = tokenize(text)
    np_obj = run_np(TokenStream(np_tokens))
    
    # Create the token
    token = VectorSpace()
    token["NP"] = 1.0  # Use correct dimension name
    token.word = f"NP({text})"
    token._original_np = np_obj
    
    # Add any additional attributes
    for key, value in attributes.items():
        token[key] = value
    
    return token


class TestSimplePPATN:
    """Test the beautifully simple Layer 3 PP ATN."""
    
    def test_preposition_plus_vector_np(self):
        """Test: PREP + vector NP = perfect PP!"""
        # Create input: ["at", NP_token([1,2,3])]
        at_token = VectorSpace()
        at_token["prep"] = 1.0
        at_token.word = "at"
        
        vector_np = create_np_token("[1,2,3]")
        
        # The PP ATN should handle this beautifully
        tokens = [at_token, vector_np]
        pp = run_pp(tokens)
        
        assert pp is not None, "PP ATN should parse PREP + NP token"
        assert pp.preposition == "at"
        assert pp.noun_phrase is not None
        print(f"✨ Simple PP: {pp}")
    
    def test_preposition_plus_simple_np(self):
        """Test: PREP + simple NP token = elegant PP!"""
        on_token = VectorSpace()
        on_token["prep"] = 1.0
        on_token.word = "on"
        
        table_np = create_np_token("the table")
        
        tokens = [on_token, table_np]
        pp = run_pp(tokens)
        
        assert pp is not None
        assert pp.preposition == "on"
        assert pp.noun_phrase.noun == "table"
        print(f"✨ Elegant PP: {pp}")
    
    def test_preposition_plus_complex_np(self):
        """Test: PREP + complex NP token = still simple!"""
        above_token = VectorSpace()
        above_token["prep"] = 1.0  
        above_token.word = "above"
        
        complex_np = create_np_token("the very large red sphere")
        
        tokens = [above_token, complex_np]
        pp = run_pp(tokens)
        
        assert pp is not None
        assert pp.preposition == "above"
        assert pp.noun_phrase.noun == "sphere"
        assert pp.noun_phrase.vector["red"] > 0
        assert pp.noun_phrase.vector["scaleX"] > 2  # very large
        print(f"✨ Complex NP, simple PP ATN: {pp}")


class TestPPATNRobustness:
    """Test that the PP ATN handles edge cases gracefully."""
    
    def test_missing_np_after_preposition(self):
        """Test PP ATN with just preposition (should fail gracefully)."""
        at_token = VectorSpace()
        at_token["prep"] = 1.0
        at_token.word = "at"
        
        tokens = [at_token]  # No NP following
        pp = run_pp(tokens)
        
        # Should fail gracefully (return None or incomplete PP)
        if pp is not None:
            assert pp.noun_phrase is None or pp.preposition == "at"
    
    def test_non_preposition_start(self):
        """Test PP ATN with non-preposition start (should fail)."""
        noun_token = VectorSpace()
        noun_token["noun"] = 1.0
        noun_token.word = "sphere"
        
        table_np = create_np_token("the table")
        
        tokens = [noun_token, table_np]  # Doesn't start with preposition
        pp = run_pp(tokens)
        
        # Should fail to parse
        assert pp is None or pp.preposition is None
    
    def test_multiple_np_tokens(self):
        """Test PP ATN behavior with multiple NP tokens (should take first)."""
        at_token = VectorSpace()
        at_token["prep"] = 1.0
        at_token.word = "at"
        
        first_np = create_np_token("the table")
        second_np = create_np_token("the chair")
        
        tokens = [at_token, first_np, second_np]
        pp = run_pp(tokens)
        
        assert pp is not None
        assert pp.preposition == "at"
        # Should use the first NP
        assert pp.noun_phrase.noun == "table"


class TestPPATNComparison:
    """Compare the old complex PP ATN vs the new simple one."""
    
    def test_simplicity_celebration(self):
        """Celebrate how much simpler Layer 3 PP ATN is!"""
        print("\n🎉 PP ATN Simplicity Celebration! 🎉")
        print("=" * 50)
        
        print("Layer 1/2 PP ATN (complex):")
        print("  PREP -> [complex NP subnetwork] -> END")
        print("  - Multiple states")
        print("  - Subnetwork calls") 
        print("  - Complex arc logic")
        print("  - Backward compatibility")
        
        print("\nLayer 3 PP ATN (simple):")
        print("  PREP -> NounPhrase token -> END")
        print("  - Just 3 states")
        print("  - Direct token consumption")
        print("  - Elegant simplicity")
        print("  - No legacy baggage")
        
        print("\n✨ The beauty of layered processing! ✨")
        
        # Demonstrate with a real example
        sentence = "above the very large red metallic sphere"
        print(f"\nExample: '{sentence}'")
        
        # Layer 3 approach
        tokens = tokenize(sentence)
        pp = run_pp(tokens)
        
        if pp:
            print(f"Result: {pp}")
            print(f"Preposition: {pp.preposition}")
            print(f"NP: {pp.noun_phrase}")
            print("✅ Parsed successfully with the simple PP ATN!")
        else:
            print("❌ Failed to parse")
        
        assert True  # This test always passes - it's a celebration!


class TestLATNLayerIntegration:
    """Test how Layer 3 integrates with the overall LATN architecture."""
    
    def test_layer_progression(self):
        """Test the beautiful progression: Layer 1 -> Layer 2 -> Layer 3."""
        sentence = "above the red box"
        
        print(f"\n🔄 LATN Layer Progression for: '{sentence}'")
        print("=" * 60)
        
        # Use the LayerExecutor for proper pipeline testing
        from engraf.lexer.latn_layer_executor import LATNLayerExecutor
        executor = LATNLayerExecutor()
        
        # Layer 1: Multi-hypothesis tokenization
        layer1_result = executor.execute_layer1(sentence)
        print(f"Layer 1: {len(layer1_result.hypotheses)} tokenization hypotheses")
        print(f"  Best: {[t.word for t in layer1_result.hypotheses[0].tokens]}")
        
        # Layer 2: NP token replacement
        layer2_result = executor.execute_layer2(sentence)
        print(f"Layer 2: {len(layer2_result.hypotheses)} NP-enhanced hypotheses")
        print(f"  Best: {[t.word for t in layer2_result.hypotheses[0].tokens]}")
        
        # Layer 3: PP token replacement  
        layer3_result = executor.execute_layer3(sentence)
        print(f"Layer 3: {len(layer3_result.hypotheses)} PP-enhanced hypotheses")
        print(f"  Best: {[t.word for t in layer3_result.hypotheses[0].tokens]}")
        print(f"  PP replacements: {len(layer3_result.hypotheses[0].pp_replacements)}")
        
        # Each layer should add value
        assert layer1_result.success
        assert layer2_result.success  
        assert layer3_result.success
        print("\n✅ Beautiful layer progression!")


